{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamline Modeling With Amazon SageMaker Studio and Amazon Experiments SDK\n",
    "\n",
    "Modeling phase is a highly iterative process in Machine Learning projects, where Data Scientists experiment with various data pre-processing and feature engineering strategies, intertwined with different model architectures, which are then trained with disparate sets of hyperparameter values. This highly iterative process, with many moving parts, can, over time, manifest into a tremendous headache in terms of keeping track of all design decisions applied in each iteration and how the training and evaluation metrics of each iteration compare to the previous versions of the model.\n",
    "\n",
    "This notebook walks you through an end-to-end example of how [Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio.html) can effectively leverage [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) to organize, track, visualize and compare our iterative work during development of a Keras model, trained to predict the age of an abalone, a sea snail, based on a set of features that describe it. While this example is specific to Keras, the same approach can be extended to other Machine Learning frameworks and algorithms. \n",
    "\n",
    "Amazon SageMaker Studio and Amazon SageMaker Experiments were unveiled at the [AWS Re:invent](https://aws.amazon.com/new/reinvent/), at the end of 2019:\n",
    "* [SageMaker Studio Announcement](https://aws.amazon.com/about-aws/whats-new/2019/12/introducing-amazon-sagemaker-studio-the-first-integrated-development-environment-ide-for-machine-learning/?trk=ls_card)\n",
    "* [SageMaker Experiments Announcement](https://aws.amazon.com/about-aws/whats-new/2019/12/introducing-amazon-sagemaker-experiments-organize-track-and-compare-your-machine-learning-training-experiments-on-amazon-sagemaker/?trk=ls_card)\n",
    "\n",
    "In this walkthrough, we will explore how Amazon SageMaker Studio, and the [Experiments SDK](https://sagemaker-experiments.readthedocs.io/en/latest/), which has been [open-sourced](https://github.com/aws/sagemaker-experiments), can be utilized to experiment with a Keras model and track data preprocessing required to prepare data for the model's consumption.\n",
    "\n",
    "Now, before we dive into hands-on exercise, let's first take a step back and discuss the building blocks of each Experiment and their referential relationships.\n",
    "* **Experiment** - a Machine Learning problem that we want to solve. Each experiment consists of a collection of Trials. Note that the name of an experiment must be unique in a given region of a particular AWS account.\n",
    "* **Trial** - an execution of a data-science workflow related to an experiment. Each Trial consists of several Trial Components. Note that the name of a trial must be unique in a given region of a particular AWS account.\n",
    "* **Trial Component** - a stage in a given trial. For instance, as we will see in our example, we will create one Trial Component for data pre-preprocessing stage and one Trial Component for model training. Similarly, in other use cases, we can also have a Trial Component for data post-processing. Unlike Experiments and Trials, Trial Components do not have to be uniquely named as they tend to represent the typical and very common stages in an ML pipeline.\n",
    "* **Tracker** - a mechanism that records various metadata about a particular Trial Component, including any Parameters, Inputs, Outputs, Artifacts and Metrics. When creating a Tracker, each Tracker is linked to a particular Training Component.\n",
    "\n",
    "![](img/experiment_structure_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Now that we've set a rock-solid foundation on the key building blocks of Experiments SDK, let's dive into the fun hands-on component of this exercise. Before we jump into data pre-processing and feature engineering, let's first set up our environment and make sure that we have all the packages imported and variables defined.\n",
    "\n",
    "First, select `Python 3 (TensorFlow 2 CPU Optimized)` kernel for your notebook in your Studio environment. The smallest instance with 2 virtual CPUs and 4 GiB of memory will be more than sufficient for our exercise.\n",
    "\n",
    "Let's begin by installing `sagemaker-experiments` package, which will enable us to work with Experiments SDK. Alongside, let's also istall `s3fs` package, to enable our pandas dataframes to easily work with objects in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: awscli 1.18.22\n",
      "Uninstalling awscli-1.18.22:\n",
      "  Successfully uninstalled awscli-1.18.22\n",
      "Found existing installation: boto3 1.12.22\n",
      "Uninstalling boto3-1.12.22:\n",
      "  Successfully uninstalled boto3-1.12.22\n",
      "\u001b[33mWARNING: Skipping s3fs as it is not installed.\u001b[0m\n",
      "Found existing installation: sagemaker-experiments 0.1.7\n",
      "Uninstalling sagemaker-experiments-0.1.7:\n",
      "  Successfully uninstalled sagemaker-experiments-0.1.7\n",
      "Collecting awscli==1.18.152\n",
      "  Using cached awscli-1.18.152-py2.py3-none-any.whl (3.4 MB)\n",
      "Collecting s3fs==0.5.0\n",
      "  Using cached s3fs-0.5.0-py3-none-any.whl (21 kB)\n",
      "Collecting sagemaker-experiments==0.1.24\n",
      "  Using cached sagemaker_experiments-0.1.24-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli==1.18.152) (0.15.2)\n",
      "Requirement already satisfied: PyYAML<5.4,>=3.10; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli==1.18.152) (5.2)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli==1.18.152) (0.4.3)\n",
      "Collecting botocore==1.18.11\n",
      "  Using cached botocore-1.18.11-py2.py3-none-any.whl (6.7 MB)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli==1.18.152) (0.3.3)\n",
      "Requirement already satisfied: rsa<=4.5.0,>=3.1.2; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli==1.18.152) (3.4.2)\n",
      "Collecting fsspec>=0.8.0\n",
      "  Using cached fsspec-0.8.3-py3-none-any.whl (88 kB)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Using cached aiobotocore-1.1.1-py3-none-any.whl (45 kB)\n",
      "Processing ./.cache/pip/wheels/84/37/2b/cf289b3d2fbebbc8a8ecafdfcc526222a9c9d1feabb3d1447c/boto3-1.15.11-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.18.11->awscli==1.18.152) (0.9.5)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore==1.18.11->awscli==1.18.152) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.18.11->awscli==1.18.152) (2.8.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=4.5.0,>=3.1.2; python_version != \"3.4\"->awscli==1.18.152) (0.4.8)\n",
      "Collecting aiohttp>=3.3.1\n",
      "  Using cached aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.6/dist-packages (from aiobotocore>=1.0.1->s3fs==0.5.0) (1.12.1)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Using cached aioitertools-0.7.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.18.11->awscli==1.18.152) (1.14.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==0.5.0) (3.0.4)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<5.0,>=4.5\n",
      "  Using cached multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (257 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n",
      "Processing ./.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13/idna_ssl-1.1.0-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.5; python_version < \"3.7\"\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==0.5.0) (2.8)\n",
      "\u001b[31mERROR: aiobotocore 1.1.1 has requirement botocore<1.17.45,>=1.17.44, but you'll have botocore 1.18.11 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore, awscli, fsspec, async-timeout, multidict, typing-extensions, yarl, attrs, idna-ssl, aiohttp, aioitertools, aiobotocore, s3fs, boto3, sagemaker-experiments\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.15.22\n",
      "    Uninstalling botocore-1.15.22:\n",
      "      Successfully uninstalled botocore-1.15.22\n",
      "Successfully installed aiobotocore-1.1.1 aiohttp-3.6.2 aioitertools-0.7.0 async-timeout-3.0.1 attrs-20.2.0 awscli-1.18.152 boto3-1.15.11 botocore-1.18.11 fsspec-0.8.3 idna-ssl-1.1.0 multidict-4.7.6 s3fs-0.5.0 sagemaker-experiments-0.1.24 typing-extensions-3.7.4.3 yarl-1.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall -y awscli boto3 s3fs sagemaker-experiments # clean up dependencies\n",
    "! pip install awscli==1.18.152 s3fs==0.5.0 sagemaker-experiments==0.1.24 # install packages required for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker as sm\n",
    "import boto3\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing as pp\n",
    "import pickle\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "role = sm.get_execution_role()\n",
    "sm_bucket = sm.session.Session().default_bucket()\n",
    "project_path = 'experiments-keras-abalone'\n",
    "source_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/'\n",
    "processed_data_path = os.path.join('s3://',sm_bucket,project_path,'data/processed/')\n",
    "artifacts_path = os.path.join(project_path,'artifacts')\n",
    "output_path = os.path.join('s3://',sm_bucket,project_path,'output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing and Feature Engineering\n",
    "\n",
    "Excellent! Now, let's dive into data pre-processing and feature engineering. First, let's pull our dataset, and its metadata, from [the archive of UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/) and familiarize with the features by reviewing the portion of the metadata file that describes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Attribute information:\n",
      "\n",
      "   Given is the attribute name, attribute type, the measurement unit and a\n",
      "   brief description.  The number of rings is the value to predict: either\n",
      "   as a continuous value or as a classification problem.\n",
      "\n",
      "\tName\t\tData Type\tMeas.\tDescription\n",
      "\t----\t\t---------\t-----\t-----------\n",
      "\tSex\t\tnominal\t\t\tM, F, and I (infant)\n",
      "\tLength\t\tcontinuous\tmm\tLongest shell measurement\n",
      "\tDiameter\tcontinuous\tmm\tperpendicular to length\n",
      "\tHeight\t\tcontinuous\tmm\twith meat in shell\n",
      "\tWhole weight\tcontinuous\tgrams\twhole abalone\n",
      "\tShucked weight\tcontinuous\tgrams\tweight of meat\n",
      "\tViscera weight\tcontinuous\tgrams\tgut weight (after bleeding)\n",
      "\tShell weight\tcontinuous\tgrams\tafter being dried\n",
      "\tRings\t\tinteger\t\t\t+1.5 gives the age in years\n"
     ]
    }
   ],
   "source": [
    "urlretrieve(source_url+'abalone.data', 'abalone.data')\n",
    "urlretrieve(source_url+'abalone.names', 'abalone.names')\n",
    "! cat abalone.names | grep 'Attribute information:' -A 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the features, let's see how many examples we have and print a few examples to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 4177 examples with 9 features and the label in the last column.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight  Rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight','Rings']\n",
    "df = pd.read_csv('abalone.data',header=None, names=col)\n",
    "print('The dataset contains {} examples with {} features and the label in the last column.\\n'.format(df.shape[0],df.shape[1]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to data transformations and feature engineering, starting with nominal features. We will apply one-hot encoding with a dropout to the nominal feature and drop the original one. We will then rename the encoded columns to ensure that their names clearly illustrate the data they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender_oh1</th>\n",
       "      <th>gender_oh2</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender_oh1  gender_oh2  Length  Diameter  Height  Whole weight  \\\n",
       "0         0.0         1.0   0.455     0.365   0.095        0.5140   \n",
       "1         0.0         1.0   0.350     0.265   0.090        0.2255   \n",
       "2         0.0         0.0   0.530     0.420   0.135        0.6770   \n",
       "3         0.0         1.0   0.440     0.365   0.125        0.5160   \n",
       "4         1.0         0.0   0.330     0.255   0.080        0.2050   \n",
       "\n",
       "   Shucked weight  Viscera weight  Shell weight  Rings  \n",
       "0          0.2245          0.1010         0.150     15  \n",
       "1          0.0995          0.0485         0.070      7  \n",
       "2          0.2565          0.1415         0.210      9  \n",
       "3          0.2155          0.1140         0.155     10  \n",
       "4          0.0895          0.0395         0.055      7  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features = [df.columns[0]]\n",
    "encoders = {}\n",
    "\n",
    "for feature in cat_features:\n",
    "    oe = pp.OneHotEncoder(drop='first')\n",
    "    oe.fit(df[[feature]])\n",
    "    encoded_feature = pd.DataFrame(oe.transform(df[[feature]]).toarray())\n",
    "    df = encoded_feature.join(df)\n",
    "    encoders[feature] = oe\n",
    "\n",
    "df.drop(columns=cat_features,inplace=True)\n",
    "df.rename(columns={0:'gender_oh1', 1:'gender_oh2'},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will tackle continuous numerical features and apply MinMaxScaler on them to confine all values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender_oh1</th>\n",
       "      <th>gender_oh2</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.521008</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.181335</td>\n",
       "      <td>0.150303</td>\n",
       "      <td>0.132324</td>\n",
       "      <td>0.147982</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.371622</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>0.079157</td>\n",
       "      <td>0.066241</td>\n",
       "      <td>0.063199</td>\n",
       "      <td>0.068261</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614865</td>\n",
       "      <td>0.613445</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.239065</td>\n",
       "      <td>0.171822</td>\n",
       "      <td>0.185648</td>\n",
       "      <td>0.207773</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.521008</td>\n",
       "      <td>0.110619</td>\n",
       "      <td>0.182044</td>\n",
       "      <td>0.144250</td>\n",
       "      <td>0.149440</td>\n",
       "      <td>0.152965</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344595</td>\n",
       "      <td>0.336134</td>\n",
       "      <td>0.070796</td>\n",
       "      <td>0.071897</td>\n",
       "      <td>0.059516</td>\n",
       "      <td>0.051350</td>\n",
       "      <td>0.053313</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender_oh1  gender_oh2    Length  Diameter    Height  Whole weight  \\\n",
       "0         0.0         1.0  0.513514  0.521008  0.084071      0.181335   \n",
       "1         0.0         1.0  0.371622  0.352941  0.079646      0.079157   \n",
       "2         0.0         0.0  0.614865  0.613445  0.119469      0.239065   \n",
       "3         0.0         1.0  0.493243  0.521008  0.110619      0.182044   \n",
       "4         1.0         0.0  0.344595  0.336134  0.070796      0.071897   \n",
       "\n",
       "   Shucked weight  Viscera weight  Shell weight  Rings  \n",
       "0        0.150303        0.132324      0.147982     15  \n",
       "1        0.066241        0.063199      0.068261      7  \n",
       "2        0.171822        0.185648      0.207773      9  \n",
       "3        0.144250        0.149440      0.152965     10  \n",
       "4        0.059516        0.051350      0.053313      7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = df.columns[2:-1]\n",
    "scalers = {}\n",
    "\n",
    "for feature in num_features:\n",
    "    mms = pp.MinMaxScaler()\n",
    "    mms.fit(df[[feature]])\n",
    "    scaled_feature = pd.DataFrame(mms.transform(df[[feature]]))\n",
    "    df[feature] = scaled_feature\n",
    "    scalers[feature] = mms\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed with splitting the pre-processed dataset for training and testing, where we will retain 80% of the dataset for training and use the remaining examples as a hold-out validation set. We will also further split the labels from the features since our Keras model, which we will review shortly, expects features and labels to be fed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train record count: 3341\n",
      "Test record count: 836\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, test and validation\n",
    "train_perc = 80\n",
    "train_cnt = int(np.floor(df.shape[0]*train_perc/100))\n",
    "\n",
    "train_df = df[:train_cnt]\n",
    "test_df = df[train_cnt:]\n",
    "\n",
    "train_features = train_df.drop(columns=['Rings'])\n",
    "train_labels = train_df['Rings']\n",
    "test_features = test_df.drop(columns=['Rings'])\n",
    "test_labels = test_df['Rings']\n",
    "\n",
    "print('Train record count: {}\\nTest record count: {}'.format(train_df.shape[0], test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's proceed with pushing the split datasets to the processed path in S3, from where the model can import them during the training. We will also combine our fitted encoders and scalers into a dictionary and serialize it as a pickle file. As we will see shortly, we will attach these fitted processors to our Trial Component that tracks data pre-processing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_csv(os.path.join(processed_data_path,'abalone_train_features.csv'),header=False, index=False)\n",
    "train_labels.to_csv(os.path.join(processed_data_path,'abalone_train_labels.csv'),header=False, index=False)\n",
    "test_features.to_csv(os.path.join(processed_data_path,'abalone_test_features.csv'),header=False, index=False)\n",
    "test_labels.to_csv(os.path.join(processed_data_path,'abalone_test_labels.csv'),header=False, index=False)\n",
    "\n",
    "with open('preprocessors.pickle','wb') as obj:\n",
    "    pickle.dump({'encoders' : encoders, 'scalers': scalers}, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's utilize the Experiments SDK to create a Pre-processing Trial Component and keep track of what've accomplished so far. We start by creating an experiment. Remember that each experiment name must be unique within a given region of a particular AWS account.\n",
    "\n",
    "Once we have our experiment instantiated, we will proceed with creating a Tracker to capture various details about the data pre-processing we completed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "ts = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "\n",
    "abalone_experiment = Experiment.create(\n",
    "    experiment_name = 'predict-abalone-age-' + ts,\n",
    "    description = 'Predicting the age of an abalone based on a set of features describing it',\n",
    "    sagemaker_boto_client=sm)\n",
    "\n",
    "\n",
    "with Tracker.create(display_name='Pre-processing', sagemaker_boto_client=sm, artifact_bucket=sm_bucket, artifact_prefix=artifacts_path) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        'train_test_split': 0.8\n",
    "    })\n",
    "    tracker.log_input(name='raw data', media_type='s3/uri', value=source_url)\n",
    "    tracker.log_output(name='preprocessed data', media_type='s3/uri', value=processed_data_path)\n",
    "    tracker.log_artifact(name='preprocessors', media_type='s3/uri', file_path='preprocessors.pickle')\n",
    "    \n",
    "processing_component = tracker.trial_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! We now have our experiment ready and we've already done our due diligence to capture our data pre-processing approach as a Trial Component. Next, let's dive into the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with SageMaker Experiments\n",
    "\n",
    "So far, we've done solid amount of work to preprocess data for our model. Before we proceed with training the model, let's take a moment to familiarize with it. We observe that our model has two fully connected hidden layers with a variable number of neurons and variable activation functions. This flexibility will enable us to easily pass these values as arguments to a training job and quickly parallelize our experimentation with several model architectures.\n",
    "\n",
    "We have Mean Squared Logarithmic Error defined as the loss function and the model will be using Adam optimization algorithm. Finally, the model will track Mean Squared Logarithmic Error as our metric, which will automatically propagate into our Training Trial Component in our Experiment, as we will see shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m''' Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\n",
      "\u001b[33m    SPDX-License-Identifier: MIT-0 '''\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Seed random number generators to ensure identical results for everyone\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mrandom\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m seed\n",
      "seed(\u001b[34m42\u001b[39;49;00m)\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m random\n",
      "random.set_seed(\u001b[34m42\u001b[39;49;00m)\n",
      "\u001b[37m########################################################################\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Sequential\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlayers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptimizers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Adam\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel\u001b[39;49;00m(x_train, y_train, x_test, y_test, args):\n",
      "    \u001b[33m\"\"\"Generate a simple model\"\"\"\u001b[39;49;00m\n",
      "    model = Sequential([\n",
      "                Dense(args.l1_size, activation=args.l1_activation, kernel_initializer=\u001b[33m'\u001b[39;49;00m\u001b[33mnormal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "                Dense(args.l2_size, activation=args.l2_activation, kernel_initializer=\u001b[33m'\u001b[39;49;00m\u001b[33mnormal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "                Dense(\u001b[34m1\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mlinear\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    ])\n",
      "\n",
      "    model.compile(optimizer=Adam(learning_rate=args.learning_rate),\n",
      "                  loss=\u001b[33m'\u001b[39;49;00m\u001b[33mmean_squared_logarithmic_error\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                  metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33mmean_squared_logarithmic_error\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    model.fit(x_train, y_train, batch_size=args.batch_size, epochs=args.epochs, verbose=\u001b[34m1\u001b[39;49;00m)\n",
      "    model.evaluate(x_test,y_test,verbose=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_training_data\u001b[39;49;00m(base_dir):\n",
      "    \u001b[33m\"\"\"Load the abalone training data\"\"\"\u001b[39;49;00m\n",
      "    x_train = (pd.read_csv(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mabalone_train_features.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))).to_numpy()\n",
      "    y_train = (pd.read_csv(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mabalone_train_labels.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))).to_numpy()\n",
      "    \u001b[34mreturn\u001b[39;49;00m x_train, y_train\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_testing_data\u001b[39;49;00m(base_dir):\n",
      "    \u001b[33m\"\"\"Load the abalone testing data\"\"\"\u001b[39;49;00m\n",
      "    x_test = (pd.read_csv(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mabalone_test_features.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))).to_numpy()\n",
      "    y_test = (pd.read_csv(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mabalone_test_labels.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))).to_numpy()\n",
      "    \u001b[34mreturn\u001b[39;49;00m x_test, y_test\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    \u001b[37m# model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m128\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--l1_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--l1_activation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--l2_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--l2_activation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args, unknown = _parse_args()\n",
      "\n",
      "    train_data, train_labels = _load_training_data(args.train)\n",
      "    eval_data, eval_labels = _load_testing_data(args.train)\n",
      "\n",
      "    abalone_regressor = model(train_data, train_labels, eval_data, eval_labels, args)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.current_host == args.hosts[\u001b[34m0\u001b[39;49;00m]:\n",
      "        \u001b[37m# save model to an S3 directory\u001b[39;49;00m\n",
      "        abalone_regressor.save(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mabalone_model.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "# Review the architecture of the model\n",
    "!pygmentize 'entrypoint.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to define a few sets of hyperparameters that we want to experiment with and kick off our training jobs to run in parallel. Note that we can also easily capture custom metrics during training by configuring a regex pattern to match the desired values in the logs generated by the training job. In our case, for demonstration purposes, we will capture the number of samples (`num_train_samples`) that the Keras framework reports as being available for training the model.\n",
    "\n",
    "Note that, in our example, we do not make any changes to our pre-processing strategy between different trials, so we will attach the same Pre-processing Trial Component that we create earlier, to each of the Trials we create in the next step.\n",
    "\n",
    "**Note:** If you elect to use your own execution role instead of the one defined in this notebook, please make sure that the access policy attached to your custom execution role, that you pass to your training job, allows `sagemaker:BatchGetMetrics` and `sagemaker:BatchPutMetrics` actions, in order for us to be able to capture non-custom metrics during training and visualize them in Studio. The four training jobs that we will kick off next will take only about 5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: abalone-train-0-2020-10-03-18-49-53-209367\n",
      "INFO:sagemaker:Creating training-job with name: abalone-train-1-2020-10-03-18-49-53-691462\n",
      "INFO:sagemaker:Creating training-job with name: abalone-train-2-2020-10-03-18-49-56-379229\n",
      "INFO:sagemaker:Creating training-job with name: abalone-train-3-2020-10-03-18-50-01-098410\n"
     ]
    }
   ],
   "source": [
    "# Define sets of hyperparameters, create a trial for each and kick of training jobs\n",
    "hyperparameters_groups=[{\n",
    "                         'learning_rate': 0.001,\n",
    "                         'epochs': 12,\n",
    "                         'batch_size': 128,\n",
    "                         'l1_size': 20,\n",
    "                         'l1_activation': 'tanh',\n",
    "                         'l2_size': 20,\n",
    "                         'l2_activation': 'relu'\n",
    "                        },\n",
    "                        {\n",
    "                         'learning_rate': 0.001,\n",
    "                         'epochs': 12,\n",
    "                         'batch_size': 128,\n",
    "                         'l1_size': 10,\n",
    "                         'l1_activation': 'tanh',\n",
    "                         'l2_size': 10,\n",
    "                         'l2_activation': 'relu'\n",
    "                        },\n",
    "                        {\n",
    "                         'learning_rate': 0.001,\n",
    "                         'epochs': 12,\n",
    "                         'batch_size': 128,\n",
    "                         'l1_size': 20,\n",
    "                         'l1_activation': 'relu',\n",
    "                         'l2_size': 10,\n",
    "                         'l2_activation': 'relu'\n",
    "                        },\n",
    "                        {\n",
    "                         'learning_rate': 0.001,\n",
    "                         'epochs': 12,\n",
    "                         'batch_size': 128,\n",
    "                         'l1_size': 10,\n",
    "                         'l1_activation': 'relu',\n",
    "                         'l1_size': 5,\n",
    "                         'l1_activation': 'relu'\n",
    "                        }]\n",
    "\n",
    "\n",
    "for i,hp_set in enumerate(hyperparameters_groups):\n",
    "\n",
    "    ts = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "    abalone_trial = abalone_experiment.create_trial(trial_name='abalone-trial-' + str(i) + '-'+ ts)\n",
    "    abalone_trial.add_trial_component(processing_component)\n",
    "    \n",
    "    abalone_estimator = TensorFlow(entry_point='entrypoint.py',\n",
    "                                   role=role,\n",
    "                                   framework_version='2.1.0',\n",
    "                                   py_version='py3',\n",
    "                                   hyperparameters=hp_set,\n",
    "                                   train_instance_count=1,\n",
    "                                   train_instance_type='ml.m5.large',\n",
    "                                   output_path = output_path,\n",
    "                                   metric_definitions=[\n",
    "                                       {'Name': 'num_train_samples', 'Regex': 'Train on (\\d+) samples'}\n",
    "                                   ]\n",
    "                                   )\n",
    "\n",
    "    job_name = 'abalone-train-' + str(i) + '-'+ ts\n",
    "    \n",
    "    abalone_estimator.fit(processed_data_path,\n",
    "                          job_name=job_name,\n",
    "                          wait=False,\n",
    "                          experiment_config={\n",
    "                                            'ExperimentName': abalone_experiment.experiment_name,\n",
    "                                            'TrialName': abalone_trial.trial_name,\n",
    "                                            'TrialComponentDisplayName': 'Training',\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Upon completion of the training jobs, we can, in just a few seconds, start visualizing how different variations of the model compare in terms of the metrics collected during model training. For instance, in just a few clicks, we can visualize how the loss has been decreasing by epoch for each variation of the model and very quickly observe the model architecture that is most effective in decreasing the loss.\n",
    "\n",
    "![](img/visualize_loss.png)\n",
    "\n",
    "You can create the same plot by following the five simple steps listed below:\n",
    "1. Select the *SageMaker Experiments List* icon on the left sidebar.\n",
    "2. Double-click on your experiment to open it and use Shift key on your keyboard to select all four trials.\n",
    "3. Right-click on any of the highlighted trials and select *Open in trial component list*.\n",
    "4. Use Shift key on your keyboard to select the four Trial Components representing the Training jobs and click on *Add chart* button.\n",
    "5. Click on *New chart* and customize it to plot the collected metrics that you would like to analyze.\n",
    "\n",
    "![](img/visualization_steps_line.gif)\n",
    "\n",
    "Similarly, we can generate a scatter plot that helps us determine whether there is a relationship between the size of the first hidden layer in the network and the Mean Squared Logarithmic Error.\n",
    "\n",
    "![](img/scatter_msle.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! How quick and effortless was that?! I encourage you to further explore plotting various other metrics on your own.\n",
    "\n",
    "Next, let's select our best performing trial (`abalone-trial-0`) and double-click on it. As expected, we see two Trial Components. One captures our data Pre-processing work, and the other reflects our Model Training.\n",
    "\n",
    "When we open the Training Trial Component, we can see that it captures all the hyper parameters, the input to the model, where the trained model was placed in S3, etc. Similarly, when we open the *Preprocessing* component, we see that it captures where the source data came from, where the processed data was stored in S3, as well as, where we can easily find our trained encoder and scalers, which we've packaged into the `preprocessors.pickle` artifact.\n",
    "\n",
    "![](img/review_artifacts.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "What a fun exploration this has been! Let's now clean up after ourselves by hierarchically deleting all elements of the experiment that we created in this notebook. The function call in the last cell has been commented out to ensure that you do not accidentally delete your experiment before getting a chance to explore it, in case if you execute the entire notebook at once (*Run All Cells* option). To delete your experiment, uncomment the last cell and execute it.\n",
    "\n",
    "I hope that you enjoyed diving into the intricacies of Experiments SDK and exploring how SageMaker Studio smoothly integrates with it, enabling you to lose yourself in experimentation with your Machine Learning model without losing any track of the hard work you've done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abalone_experiment.delete_all('--force')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
